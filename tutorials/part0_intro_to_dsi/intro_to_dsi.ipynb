{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "import sys\n",
    "import pyemu\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "sys.path.insert(0,\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "“DSI” stands for “data space inversion”. Data space inversion (DSI) enables the exploration of a model prediction's posterior distribution without requiring the exploration of the posterior distribution of model parameters. This is achieved by constructing a surrogate model using principal component analysis (PCA) of the covariance matrix of model outputs (i.e., observations). This matrix links model outputs corresponding to field measurements with predictions of interest. The resulting predictions are then conditioned on real-world measurements of system behavior.\n",
    "\n",
    "The general idea is to:\n",
    "1. Generate an ensemble of model outputs simulated using with the prior. \n",
    "2. Covariance is empirically obtained from the ensemble of model outputs.\n",
    "3. The surrogate model is constructed and conditioned against measured data.\n",
    "4. A sample of the prediction posterior distribution is obtained.\n",
    "\n",
    "The following notebook goes through the the method described by [Sun and Durlofsky (2017)](https://doi.org/10.1007/s11004-016-9672-8) and [Lima et al (2020)](https://doi.org/10.1007/s10596-020-09933-w).\n",
    "\n",
    "\n",
    "# Generate the prior observation ensemble\n",
    "\n",
    "First we need some \"training data\". Let us start by cooking up some fake \"model outputs\". Say we have a \"model\" that outputs three \"measured\" observations and a \"prediction\". Let us say we have run our model with a 1000 samples of the prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean values for each variable\n",
    "mean = [0, 1, 2, 3]\n",
    "\n",
    "# Covariance matrix: answer at the back of the book...\n",
    "true_cov = [\n",
    "    [1, 0.8, 0.5, 0.5],  \n",
    "    [0.8, 1, 0.3, 0.3],  \n",
    "    [0.5, 0.3, 1, .2],   \n",
    "    [0.5, 0.3,.2,1]\n",
    "]\n",
    "\n",
    "# Number of samples to generate a.k.a. ensemble size\n",
    "nreal = 1000\n",
    "\n",
    "# Generate the fake prior observation ensemble\n",
    "fake_sim_ensemble = pd.DataFrame(np.random.multivariate_normal(mean, true_cov, nreal),\n",
    "                                 columns=[\"prediction\",\"obs1\",\"obs2\",\"obs3\"])\n",
    "fake_sim_ensemble.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-space inversion\n",
    "\n",
    "Following the notation in [Lima et al (2020)](https://doi.org/10.1007/s10596-020-09933-w), $\\mathbf{d}$ is the vector of model simulated outputs that contains both predictions and measurements. As mentioned above, the main idea behind the method is to use PCA to write the vector of predicted data ($\\mathbf{d}_{\\text{PCA}}$) as:\n",
    "\n",
    "$$\n",
    "\\mathbf{d}_{\\text{PCA}} = \\bar{\\mathbf{d}} + \\mathbf{C}_d^{1/2} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "in which $\\bar{\\mathbf{d}}$ and $\\mathbf{C}_d$ are the mean and the covariance matrix of  $\\mathbf{d}$, and $\\mathbf{x}$ is a vector of random numbers. Both of which are obtained from the ensemble of model outputs.\n",
    "\n",
    "## Calculate the mean-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mean\n",
    "d_bar = fake_sim_ensemble.mean()\n",
    "d_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this is an approximation of the `true_cov` matrix\n",
    "Cd = fake_sim_ensemble.cov()\n",
    "Cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate $\\mathbf{C}_d^{1/2}$\n",
    "\n",
    "$\\mathbf{C}_d$ is calcualted as:\n",
    "\n",
    "$$\n",
    "\\mathbf{C}_d = \\frac{1}{N-1} \\sum_{i=1}^{N} (\\mathbf{d}_i - \\bar{\\mathbf{d}}) (\\mathbf{d}_i - \\bar{\\mathbf{d}})^T\n",
    "$$\n",
    "\n",
    "where $N$ is the number of samples in the ensemble, $\\mathbf{d}_i$ is the $i$-th sample of the ensemble, and $\\bar{\\mathbf{d}}$ is the mean of the ensemble.\n",
    "Which is equivalent to:\n",
    "\n",
    "$$\n",
    "\\mathbf{C}_d = \\Delta\\mathbf{D} \\Delta\\mathbf{D}^T\n",
    "$$\n",
    "\n",
    "where $\\Delta\\mathbf{D}$ is the matrix of the ensemble of model outputs with the mean subtracted from each row, and is calculated as:\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{D} = \\frac{1}{\\sqrt{N_e - 1}} \\left[ \\mathbf{d}_1 - \\bar{\\mathbf{d}}, \\ldots, \\mathbf{d}_{N_e} - \\bar{\\mathbf{d}} \\right].\n",
    "$$\n",
    "\n",
    "where $N_e$ is the number of samples in the ensemble.\n",
    "\n",
    "Let's calculate $\\Delta \\mathbf{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: to maintain consistency with notation used in the papers, \n",
    "# here we need to transpose our ensemble to be of shape (nobs,nreal)\n",
    "deltaD = fake_sim_ensemble.T.apply(lambda x: (x - x.mean()) / np.sqrt(fake_sim_ensemble.shape[0]-1),axis=1)\n",
    "deltaD.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we talking about $\\Delta\\mathbf{D}$? Because $\\mathbf{C}_d^{1/2}$, used in the first equation we presented, is calculated using the singular value decomposition (SVD) of $\\Delta\\mathbf{D}$:\n",
    "\n",
    "$$\n",
    "\\Delta\\mathbf{D} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n",
    "$$\n",
    "\n",
    "where $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal matrices and $\\mathbf{\\Sigma}$ is a diagonal matrix with the singular values of $\\Delta\\mathbf{D}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD\n",
    "U, Sigma, Vt = np.linalg.svd(deltaD, full_matrices=False)\n",
    "U.shape,Sigma.shape,Vt.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From these, we can now calculate the square root of $\\mathbf{C}_d$ as:\n",
    "\n",
    "$$\n",
    "\\mathbf{C}_d^{1/2} = \\mathbf{U} \\mathbf{\\Sigma}\n",
    "\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\Sigma}^{1/2}$ is a diagonal matrix with the square root of the singular values of $\\Delta\\mathbf{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cd_sqrt = np.dot(U,np.diag(Sigma)) #eq 14 in Lima 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model emulator\n",
    "\n",
    "The emulator is nothing more than a linear transformation of the model outputs. The emulator is constructed by projecting the model outputs onto the principal components of the covariance matrix of the model outputs. \n",
    "\n",
    "The model emulator is \"run\" by calculating $\\bar{\\mathbf{d}} + \\mathbf{C}_d^{1/2} \\mathbf{x}$  The values of $\\mathbf{x}$ will be \"PEST adjustable parameters\" that are sampled from a normal distribution with mean of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"prior\" mean of emulator \"parameters\"\n",
    "x = np.zeros_like(Sigma)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a model-emulator \"forward run\"\n",
    "d_bar.values + np.dot(Cd_sqrt,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy calibration\n",
    "\n",
    "In practice, how do we handle this with PEST? The $\\bar{\\mathbf{d}}$ and $\\mathbf{C}_d^{1/2}$ matrices are constructed and recorded in the PEST mocel directory. Then, a forward run script is prepared which reads these matrices, as well as the PEST-adjusted values of the vector $\\mathbf{x}$, and calculates the model emulator outputs. \n",
    "\n",
    "Let's demonstrate this with a simple example. Here is what a forward run might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_run(x):\n",
    "    #pretend to read d_bar\n",
    "    #pretend to read Cd_sqrt\n",
    "    #pretend to read x\n",
    "    return d_bar.values + np.dot(Cd_sqrt, x)\n",
    "\n",
    "x = np.zeros_like(Sigma)\n",
    "obs = forward_run(x)\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now lets choose a \"truth\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a realisationas the truth\n",
    "truth = fake_sim_ensemble.loc[0]\n",
    "truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and now calibrate the emulator to the truth observations (don't do this at home folks...this only works well because it is a super simple example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the pvals that minimize the difference between the truth and the forward run\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def objective(x):\n",
    "    # objective does not include the prediction column\n",
    "    return np.sum((forward_run(x)[1:] - truth[1:])**2)\n",
    "\n",
    "# intial parameters\n",
    "pvals_guess = np.zeros_like(Sigma)\n",
    "\n",
    "# optimize\n",
    "res = minimize(objective, pvals_guess,tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(4,4))\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(truth,forward_run(x), label='initial parameters')\n",
    "ax.scatter(truth,forward_run(res.x), label='calibrated parameters')\n",
    "ax.set_ylabel('simulated')\n",
    "ax.set_xlabel('measured')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "#add one to one line\n",
    "ax.plot([truth.min(),truth.max()],[truth.min(),truth.max()],'k--',alpha=0.3);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmdsitut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
