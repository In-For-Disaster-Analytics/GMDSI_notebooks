{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "import psutil\n",
    "\n",
    "import sys\n",
    "import pyemu\n",
    "import flopy\n",
    "assert \"dependencies\" in flopy.__file__\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-space Inversion\n",
    "\n",
    "Data space inversion (DSI) enables the exploration of a model prediction's posterior distribution without requiring the exploration of the posterior distribution of model parameters. This is achieved by constructing a surrogate model using principal component analysis (PCA) of a covariance matrix of model outputs. This matrix links model outputs corresponding to field measurements with predictions of interest. The resulting predictions are then conditioned on real-world measurements of system behavior in the latent PCA subspace - whew!\n",
    "\n",
    "tl;dr: DSI is a surrogate modelling approach that works by mapping statistical relationships between observations. Its super fast and relatively robust. It is good for uncertainty quantification, data assimilation and even optimization.\n",
    "\n",
    "DSI can be an efficient tool for predictive uncertainty quantification, as it allows for the exploration of the uncertainty in model predictions without the need to explore the uncertainty in model parameters. This is particularly useful when the model is complex and the parameter space is high-dimensional. DSI can also be used to explore the sensitivity of model predictions to different types of data, and to identify the most informative data types for reducing predictive uncertainty. \n",
    "\n",
    "See the \"intro to EVA and DSI\" notebook for a more detailed explanation of the method, as well as the original papers [Sun and Durlofsky (2017)](https://doi.org/10.1007/s11004-016-9672-8), and subsequent variations (e.g., [Lima et al 2020](https://doi.org/10.1007/s10596-020-09933-w)). There are also [GMDSI webinars and lectures available on YouTube](https://www.youtube.com/watch?v=s2g3HaJa1Wk&t=1564s).\n",
    "\n",
    "In these notebooks we will focus on how to implement DSI for predictive uncertainty quantification, using pyEMU and PEST++. We will not discuss the maths behind the method in detail, but rather focus on how to use the tools to implement it. For a review of the maths see the [\"intro to eva and dsi\"](../part0_intro_to_dsi/intro_to_dsi.ipynb) notebook.\n",
    "\n",
    "## Getting ready\n",
    "\n",
    "Undertaking DSI relies on the existence of an ensemble of model-generated outputs (i.e., observations in the pest control file) for both historical observation quantities (eg heads, flows, concentrations, etc) AND forecast quantities of interest - this is important so we will say it again: DSI requires the results of a Monte Carlo set of runs for both historic and future/scenario (prediction) conditions. These results are generated by running the model with a range of parameter values, which are usually sampled from the prior parameter distribution. Note that this distribution does not need to be Gaussian, and each model \"parameterization\" can be as complex as the user desires. Generating the the combined historic-future/scenario output ensemble is the only time that the numerical model needs to be run. Ideally, the ensemble size should be as large as you can afford. However, once generated, the DSI data-driven/emulator \"model\" runs very quickly.\n",
    "\n",
    "Let us start by generating the ensemble of model outputs. We will make use of the prior ensemble generated in a previous tutorial. The next couple of cells load necessary dependencies and call a convenience function to prepare the PEST dataset folder for you. This is the same dataset that was constructed during the [\"freyberg ies\"](../part2_06_ies/freyberg_ies_1_basics.ipynb) tutorial. Simply press `shift+enter` to run the cells.\n",
    "\n",
    "Note: The DSI method is very useful for more computationally expensive models. In our tutorial case, there is not actually much computational advantage in using the emulator (i.e., DSI) versus using the numerical model (i.e. Freyberg model). This is because the Freyberg model is very fast already. Even so, DSI is faster...specially when using pyEMU, as we can run all of the workers in memory, without needing to read/write to disk! We will demonstrate this with the new PyWorker functionality available in pyEMU.\n",
    "\n",
    "..anyway...lez'go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the template directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the temporary working folder\n",
    "ies_d = os.path.join('master_ies_1a')\n",
    "if os.path.exists(ies_d):\n",
    "    shutil.rmtree(ies_d)\n",
    "\n",
    "org_t_d = os.path.join(\"..\",\"part2_06_ies\",\"master_ies_1a\")\n",
    "if not os.path.exists(org_t_d):\n",
    "    raise Exception(f\"you need to run the {org_t_d} notebook\")\n",
    "shutil.copytree(org_t_d,ies_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us quickly load the Pst control file and remind ourselves of observations and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_freyberg = pyemu.Pst(os.path.join(ies_d, \"pest.pst\"))\n",
    "pst_freyberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the observation ensembles for use in plotting later\n",
    "oe_pr = pst_freyberg.ies.obsen0.copy()\n",
    "oe_pt = pst_freyberg.ies.get(\"obsen\",pst_freyberg.control_data.noptmax).copy()\n",
    "oe_pr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a quick look at the pestpp options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_freyberg.pestpp_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pst_freyberg.pestpp_options['forecasts'].split(',')\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading the prior observation ensemble. Check how many realisations it has. These are our \"training data\". Let's use all of them. (if you like, you can experiment with what happens by using less realizations in the training dataset...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pst_freyberg.ies.obsen0.copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous notebooks, we set the observation weights to be \"balanced for visibility\". Thus they do not reflect a measure of measurement noise. However, we included the `standard_deviation` column in the PEST observation data section. Ensembles of observation noise are generated using the latter.\n",
    "\n",
    "Experience suggests that, when conditioning DSI there is not much advantage in weighting for visibility. Achieving model to measurement fits commensurate with noise is rarely an issue with these methods. In practice, it may be more convenient to assign weights explicitly as the inverse of the standard deviation of noise. This provides an easy way to verify if over-fitting is occurring (i.e Phi should be >= than the number of non-zero obs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst_freyberg.observation_data\n",
    "obs.loc[obs.weight>0, ['obsval','weight','standard_deviation']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change our observation weights to be the inverse of the standard deviation of noise. Alternatively we could explicitly pass in an ensemble of observation noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.loc[obs.weight>0,\"weight\"] = 1.0 / obs.loc[obs.weight>0,\"standard_deviation\"]\n",
    "assert obs.weight.sum()>0, \"no non-zero obs weights found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.oname.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from previous notebooks that we are carrying along a whole bunch of zero-weighted observations. In principle, it is feasible to carry over all of these to DSI. In practice, you probably want to only carry over as much as you really need. This will keep DSi forward run as fast as possible and provide the PCA more useful degrees of freedom. Lets drop everything except nzobs and predictions to keep this super-light:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in pst_freyberg.instruction_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop obs to ncols\n",
    "drop_list = [f for f in pst_freyberg.instruction_files if f.startswith(\"hdslay\")]\n",
    "drop_list.extend([f for f in pst_freyberg.instruction_files if \".npf_k_layer1.\" in f])\n",
    "drop_list.append(\"inc.csv.ins\")\n",
    "drop_list.append(\"cum.csv.ins\")\n",
    "drop_list.extend([f for f in pst_freyberg.instruction_files if \".tdiff.\" in f])\n",
    "\n",
    "for o in drop_list:\n",
    "    pst_freyberg.drop_observations(os.path.join(ies_d,o),pst_path='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the observation ensembles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsnmes = pst_freyberg.observation_data.obsnme.values\n",
    "\n",
    "data = data.loc[:, obsnmes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the DSI model and PEST setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pyEMU, `pyemu.emulators` is the entry point for all things emulation. The minimum requirement to instantiate a `DSI` object is the training data set. When initialized it prepares in memory the various components required for DSI and associated analyses. \n",
    "\n",
    "Optionaly you can specify the energy level truncation for SVD, data transfromations (we will ge tto this later) and an existing `Pst` object from the full-order model. `DSI` will use oinfromation in the `Pst` to help construct a dsi pestpp template directory later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyemu.emulators import DSI\n",
    "\n",
    "dsi = DSI(pst=pst_freyberg,#optional\n",
    "          data=data,\n",
    "          transforms=None, #optional\n",
    "          energy_threshold=1., #optional\n",
    "          verbose=True)\n",
    "dsi.fit();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check on the various attributes like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsi.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And access them like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsi.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the dsi model directly, you can call `dsi.predict()` and passing in a `pvals` array (i.e., a vector of random normal values with shape equal to `dsi.s`). This is effectively the dsi \"forward run\". When we setup pestpp, we are parameterizing the `pvals` vector and allowing pestpp to adjust the values to improive the fit with observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the singular values\n",
    "dsi.s[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = np.random.normal(0,1,dsi.s.shape)\n",
    "dsi.predict(pvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is where things get fancy. We also need to construct a new PEST setup to condition the surrogate. Luckily, pyEMU has a function that does all the hard work for us!\n",
    "\n",
    "When calling `.prepare_pestpp()` on the `DSI` object, pyemu will prepare a Pst object and folder with all the files required to run and condition the surrogate model using pestpp-ies. The new PEST control file is named `dsi.pst`. It contains all the observations that were included in the Pst object passed to `DSI`. Parameters in the new control file are the vector of $\\mathbf{x}$, i.e. the surrogate model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = \"dsi_template\"\n",
    "pst = dsi.prepare_pestpp(t_d=t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One extra step for our case, because `.prep_for_dsi` does not copy over executables (note we dont need MODFLOW for DSI!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pst_template copy exe files\n",
    "found = False\n",
    "for f in os.listdir(os.path.join(ies_d)):\n",
    "    if f.startswith(\"pestpp-ies\"):\n",
    "        shutil.copy2(os.path.join(ies_d,f),os.path.join(t_d,f))\n",
    "        found = True\n",
    "if not found:\n",
    "    raise Exception(\"couldn't find pestpp-ies binary in {0}\".format(ies_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at what's in this new folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the various components of the `dsi.pst` version 2 control file. Files that start with `dsi.` are the model emulator input and output files. \n",
    "\n",
    "The `DSI` class was pickled and stored for later use. \n",
    "The `dsi_sim_vals.csv` file contains the emulator generated observation values using the input parameters, $\\mathbf{x}$, contained in `dsi_pars.csv`. \n",
    "The `forward_run.py` script contains code to read input files, run the emulator and record outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at how npar has changed. The number of parameters in the original Pst object is the number of parameters in the model. The number of parameters in the new Pst object is the number of principal components used in the emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.npar_adj, pst_freyberg.npar_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify we have the same number of observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.nobs, pst_freyberg.nobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.nnz_obs, pst_freyberg.nnz_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important aspect in any emulation workflow is the idea of \"extrapolation\". It is usualy a bad idea to use an emulator to extrapolate beyond the range of training data. Note that, we do provide an option to do so. However, it is usualy not recommended. If extrapolation is necessary, it is probably a better idea to extend the trining data set. To protect against #badtimes we recommend always using prior data conflict resolution when conditioning a DSI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"ies_drop_conflicts\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right on! We are ready to get cracking. Let's run pestpp-ies and see what we get.\n",
    "\n",
    "#ATTENTION!\n",
    "\n",
    "As always, set the number of workers according to your resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the number of realizations and how many workers you have at your disposal, the next cell may take a while to run. Although the emulator model is super fast, we are running it many times.\n",
    "\n",
    "As always, ideally you would want as many realizations as you can afford. For the sake of the tutorial let's stick to 200. (The next cell might take 10-20min to run, depending on the number of workers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets specify the number of realizations to use; usually this should be as many as you can afford\n",
    "num_reals = data.shape[0] #using few 'cause its a tutorial and dont want to take too long...\n",
    "pst.pestpp_options[\"ies_num_reals\"] = num_reals\n",
    "\n",
    "# set noptmax\n",
    "pst.control_data.noptmax = 3\n",
    "\n",
    "# and re-write\n",
    "pst.write(os.path.join(t_d,\"dsi.pst\"),version=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to introduce a new way to deploy pestpp: the `PyWorker`. This option is only available when working with python/pyemu. Because DSI is very fast, reading and writting files to disk becomes the bottleneck to running it many times. And because we are impactient, we want to make it as fast as possible. Using `PyWorkers` allows us to run all the workers directly in memmory, avoiding disk i/o. As our dsi implementation does not actualy need to read/write files to disk.\n",
    "\n",
    "To do so, we use the same `pyemu.os_utils.start_workers` function as we always do, but with 2 new arguments: `ppw_function` and `ppw_kwargs`. These efectilvely tell the worker what function it needs to run (and the arguments that that function takes). pyemu has inbuilt pyworker functions for our implmentation of dsi. So all you need to worry about is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the master dir\n",
    "m_d = \"master_dsi\"\n",
    "\n",
    "num_workers = 15\n",
    "\n",
    "pvals = pd.read_csv(os.path.join(t_d, \"dsi_pars.csv\"), index_col=0)\n",
    "\n",
    "\n",
    "pyemu.os_utils.start_workers(\n",
    "    t_d,\"pestpp-ies\",\"dsi.pst\", num_workers=num_workers,\n",
    "    worker_root=\".\", master_dir=m_d, \n",
    "    ppw_function=pyemu.helpers.dsi_pyworker,\n",
    "    ppw_kwargs={\n",
    "        \"dsi\": dsi, \"pvals\": pvals,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right on. That was fast! (should be <1min)\n",
    "\n",
    "Let's check the evolution of the objective function (remember - this is the same objective function - observations and weights - that we have been using in the other notebooks on data assimilation/history matching!). We should see it decreasing as the emulator is conditioned/trained on the observations, but we don't want it to become less than the number of nonzero-weighted observations (`nnzobs`) #overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(m_d,\"dsi.pst\"))\n",
    "\n",
    "phidf = pst.ies.phiactual\n",
    "\n",
    "fig,ax=plt.subplots(1,1,figsize=(4,4))\n",
    "ax.plot(phidf.index,phidf['mean'],\"bo-\", label='dsi')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('phi')\n",
    "ax.set_xlabel('iteration')\n",
    "\n",
    "ax.text(0.7,0.9,f\"nnz_obs: {pst.nnz_obs}\\nphi_dsi: {phidf['mean'].iloc[-1]:.2f}\",\n",
    "        transform=ax.transAxes,ha=\"right\",va=\"top\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as usual, we bring this back to the predictions. How has DSI performed? While we are at it, let's compare to the numerical model derived predictions we obtained in the \"freyberg ies\" notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dsi_hist(m_d,pst,iteration=3):\n",
    "    pr_oe_dsi = pst.ies.obsen0.copy()\n",
    "    pt_oe_dsi = pst.ies.get(\"obsen\",iteration).copy()\n",
    "\n",
    "\n",
    "    fig,axes = plt.subplots(len(predictions)//2,2,figsize=(7,7))\n",
    "    for p,ax in zip(predictions,axes.flatten()):\n",
    "            #calculate consistent bin edges\n",
    "            bins = np.linspace(\n",
    "                    min(oe_pr.loc[:,p].values.min(),\n",
    "                        oe_pt.loc[:,p].values.min(),\n",
    "                        pr_oe_dsi.loc[:,p].values.min(),\n",
    "                        pt_oe_dsi.loc[:,p].values.min()),\n",
    "                    max(oe_pr.loc[:,p].values.max(),\n",
    "                        oe_pt.loc[:,p].values.max(),\n",
    "                        pr_oe_dsi.loc[:,p].values.max(),\n",
    "                        pt_oe_dsi.loc[:,p].values.max()),\n",
    "                    30)\n",
    "\n",
    "            ax.hist(oe_pr.loc[:,p].values,alpha=0.5,facecolor=\"0.5\",density=True,label=\"prior\",bins=bins)\n",
    "            ax.hist(oe_pt.loc[:, p].values,  alpha=0.5, facecolor=\"b\",density=True,label=\"posterior\",bins=bins)\n",
    "            ax.hist(pr_oe_dsi.loc[:, p].values,  facecolor=\"none\",hatch=\"/\",edgecolor=\"0.5\",lw=0.5,density=True,label=\"dsi prior\",bins=bins)\n",
    "            ax.hist(pt_oe_dsi.loc[:, p].values,  facecolor=\"none\",density=True,hatch=\"/\",edgecolor=\"b\",lw=.5,label=\"dsi posterior\",bins=bins)\n",
    "            \n",
    "            fval = pst.observation_data.loc[p,\"obsval\"]\n",
    "            ax.plot([fval,fval],ax.get_ylim(),\"r-\",label='truth')\n",
    "            \n",
    "            ax.set_title(p,loc=\"left\")\n",
    "            ax.legend(loc=\"best\")\n",
    "            ax.set_yticks([])\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration=1\n",
    "plot_dsi_hist(m_d,pst,iteration=iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby. Prediction variance has decreased for most forecasts (blue histograms have less spread than grey ones). And the posterior distributions all capture the truth. \n",
    "\n",
    "There is a noticeable issue for the particle travel time prediction: the model emulator is predicting physically impossible values (i.e. negative times). On top of that, the physics-based model prior and posterior show a distinctly skewed distribution, whilst the emulator derived output are symmetric. This is due to the Gaussian assumption of the emulator. To avoid it, we can  apply transformation of the observation.\n",
    "\n",
    "First, lets look at another common issue: saw-tooth patterns in time-series outputs. Make a quick plot of the future time series outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tseries_ensembles(pt_oe, onames=[\"hds\",\"sfr\"],noise_oe=None):\n",
    "    pst.try_parse_name_metadata()\n",
    "    # get the observation data from the control file and select \n",
    "    obs = pst.observation_data.copy()\n",
    "    # onames provided in oname argument\n",
    "    obs = obs.loc[obs.oname.apply(lambda x: x in onames)]\n",
    "    # only non-zero observations\n",
    "    obs = obs.loc[obs.obgnme.apply(lambda x: x in pst.nnz_obs_groups),:]\n",
    "    # make a plot\n",
    "    ogs = obs.obgnme.unique()\n",
    "    fig,axes = plt.subplots(len(ogs),1,figsize=(7,2*len(ogs)))\n",
    "    ogs.sort()\n",
    "    # for each observation group (i.e. timeseries)\n",
    "    for ax,og in zip(axes,ogs):\n",
    "        # get values for x axis\n",
    "        oobs = obs.loc[obs.obgnme==og,:].copy()\n",
    "        oobs.loc[:,\"time\"] = oobs.time.astype(float)\n",
    "        oobs.sort_values(by=\"time\",inplace=True)\n",
    "        tvals = oobs.time.values\n",
    "        onames = oobs.obsnme.values\n",
    "        ax.plot(oobs.time,oobs.obsval,color=\"fuchsia\",lw=2,zorder=99)\n",
    "        # plot prior\n",
    "        #[ax.plot(tvals,pr_oe.loc[i,onames].values,\"0.5\",lw=0.5,alpha=0.5) for i in pr_oe.index]\n",
    "        # plot posterior\n",
    "        [ax.plot(tvals,pt_oe.loc[i,onames].values,\"b\",lw=0.5,alpha=0.5) for i in pt_oe.index]\n",
    "        # plot measured+noise \n",
    "        oobs = oobs.loc[oobs.weight>0,:]\n",
    "        tvals = oobs.time.values\n",
    "        onames = oobs.obsnme.values\n",
    "        if noise_oe is not None:\n",
    "            [ax.plot(tvals,noise_oe.loc[i,onames].values,\"r\",lw=0.5,alpha=0.5,zorder=0) for i in noise_oe.index]\n",
    "        #[ax.plot(tvals,noise.loc[i,onames].values,\"r\",lw=0.5,alpha=0.5,zorder=0) for i in noise.index]\n",
    "        ax.plot(oobs.time,oobs.obsval,\"r-o\",lw=2,zorder=100)\n",
    "        ax.set_title(og,loc=\"left\")\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell plots the posterior of three forecast time-series. As you can see, DSI generated outputs are nice and smooth in the historical period. However, the same cannot be said for the forecast period, where a \"saw-toothed\" behaviour can be seen, with the time-series jumping up and down. It is specially evident in the early-future for site `trgw-0-3-8`. This may also be somewhat mitigated by using normal-score transformation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_oe_dsi = pst.ies.get(\"obsen\",iteration)\n",
    "fig = plot_tseries_ensembles(pt_oe_dsi, onames=[\"hds\",\"sfr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured noise\n",
    "\n",
    "As you saw, overfitting is too easy. As we discussed in the `ies_4_noise` notebook, adding structure to the noise ensemble can help be helpfull. Lets do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate standard normal deviates\n",
    "num_reals = pst.ies.paren0.shape[0]\n",
    "np.random.seed(pyemu.en.SEED)\n",
    "draws = np.random.normal(0,1,num_reals)\n",
    "#_ = plt.hist(draws)\n",
    "\n",
    "obs = pst.observation_data\n",
    "onames = obs.loc[(obs.weight>0) & (obs.oname.apply(lambda x: x in [\"hds\",\"sfr\"])),\"obsnme\"]\n",
    "# first generate a standard noise ensemble \n",
    "newnoise = pyemu.ObservationEnsemble.from_gaussian_draw(pst=pst,cov=pyemu.Cov.from_observation_data(pst),num_reals=num_reals)\n",
    "newnoise.index = pst.ies.paren0.index\n",
    "ovals =  obs.loc[onames,\"obsval\"].values\n",
    "stdevs = obs.loc[onames,\"standard_deviation\"].values\n",
    "for i,draw in enumerate(draws):\n",
    "    newnoise.loc[i,onames] = ovals + (draw * stdevs)\n",
    "if \"base\" in newnoise.index:\n",
    "    newnoise.loc[\"base\",:] = obs.loc[newnoise.columns,\"obsval\"]\n",
    "newnoise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newnoise.to_csv(os.path.join(t_d,\"extreme_noise.csv\"))\n",
    "pst.pestpp_options[\"ies_observation_ensemble\"] = \"extreme_noise.csv\"\n",
    "pst.write(os.path.join(t_d,\"dsi.pst\"),version=2)\n",
    "\n",
    "\n",
    "# the master dir\n",
    "m_d = \"master_dsi_strctnoise\"\n",
    "pvals = pd.read_csv(os.path.join(t_d, \"dsi_pars.csv\"), index_col=0)\n",
    "\n",
    "pyemu.os_utils.start_workers(\n",
    "    t_d,\"pestpp-ies\",\"dsi.pst\", num_workers=num_workers,\n",
    "    worker_root=\".\", master_dir=m_d, \n",
    "    ppw_function=pyemu.helpers.dsi_pyworker,\n",
    "    ppw_kwargs={\n",
    "        \"dsi\": dsi, \"pvals\": pvals,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(m_d,\"dsi.pst\"))\n",
    "\n",
    "phidf = pst.ies.phiactual\n",
    "\n",
    "fig,ax=plt.subplots(1,1,figsize=(4,4))\n",
    "ax.plot(phidf.index,phidf['mean'],\"bo-\", label='dsi')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('phi')\n",
    "ax.set_xlabel('iteration')\n",
    "\n",
    "ax.text(0.7,0.9,f\"nnz_obs: {pst.nnz_obs}\\nphi_dsi: {phidf['mean'].iloc[-1]:.2f}\",\n",
    "        transform=ax.transAxes,ha=\"right\",va=\"top\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much difference in the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration=1\n",
    "plot_dsi_hist(m_d,pst,iteration=iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering and Data transformation\n",
    "\n",
    "Ideally, DSI training-data should be pluri-Gaussian. We can not esnure that requirement, but we can try to ensure that at least each feature (observation) is aproximately Gaussian. We acomplish by transforming data and calcualting the covariance between trasnformed observations. Transformed observation values will have a distribution which is more similar to Gaussian, thus better respecting the assumptions on which DSI relies. Then, during the DSI forward run, emulator-derived outputs must be back-transformed into the original data space, so as to allow for direct comparison to measured values.\n",
    "\n",
    "In the `pyemu` python implementation this comes at a slight cost of increased run time, both in setting up the emulator, as well as in the forward run. The latter may cost a couple of fractions of a second more.  If you are transfroming many observations, this can add up. Sorry about that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at some of the observations statistical distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,predictions].hist(figsize=(4,4))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`part_time` is a clear candidate for transformation in the predictions, with a distinct log-normal distribution. We should probalby apply a log-transformation to this feautre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at some non-zero obs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,obs.loc[obs.weight>0,\"obsnme\"].tolist()].hist(figsize=(10,10))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit more chalenging here. Most of the `hds` observations are pretty close to normal. They might benefit somewhat from normal-score transformation, but generally they are already pretty close. \n",
    "\n",
    "More of a chalenge are the `gage-1` observations. At first glance it may appear that they too are close to log-normal. Unfortunatley, that is not quite the case. You see, these observations are \"bounded\" at zero. Flow through the gage can never be <0, so we end up with a whole bunch of realizations with a value of \"zero\". (See figure below). Transforming these types of distributions is chalenging. Doing so requires some dirty tricks in the background, which sometimes hinder DSI performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = data.loc[:,obs.loc[obs.weight>0,\"obsnme\"].tolist()[-1]].values\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.hist(np.log10(vals+vals.min()+.1), bins=30)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "An alternative approach is to throw out realizations that duplicate values at the bounds...but this means losing training data. Sub-optimal, but if you can afford to generate more training data, it can be worth this type of sample rejection to create a cleaner training dataset (or, preferably build this conceptual knowledge into the prior).\n",
    "\n",
    "So we know something: we have measured values for `gage-1`. We know these do not reach zero; in fact they dont seem to go below 1000. And looking at the prior distribution, it appears that we have a bunch of \"outliers\" with flows below 100 (2 on the log10 scale). So we can perhaps throw out realizations where flows during the historical period are below 100 as a form of \"prior conditioning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsnmes = obs.loc[(obs.weight>0) & (obs.usecol==\"gage-1\"),\"obsnme\"].tolist()\n",
    "keep_reals = data.loc[~data.loc[:,obsnmes].lt(100).any(axis=1)].index.tolist()\n",
    "print(\"keep reals:\", len(keep_reals))\n",
    "o = obs.loc[obs.weight>0,\"obsnme\"].tolist()[-1]\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "\n",
    "vals = data.loc[:,o].values\n",
    "logvals = np.log10(vals+vals.min()+1e-6)\n",
    "bins = np.linspace(logvals.min(),logvals.max(),50)\n",
    "plt.hist(logvals, bins=bins,label='full prior',color='0.5')\n",
    "vals = data.loc[keep_reals,o].values\n",
    "logvals = np.log10(vals+vals.min()+1e-6)\n",
    "plt.hist(logvals, bins=bins,label='reduced prior',color='b',alpha=0.3)\n",
    "plt.vlines(np.log10(obs.loc[o].obsval+vals.min()+1e-6), 0, 30, color='r',label=\"measured\")\n",
    "plt.legend(fontsize=8,loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! That looks alot better. Pretty close to normal as it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[keep_reals,obs.loc[obs.weight>0,\"obsnme\"].tolist()].hist(figsize=(10,10))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = flopy.mf6.MFSimulation.load(sim_ws=ies_d,verbosity_level=0)\n",
    "gwf = sim.get_model()\n",
    "top = gwf.dis.top.get_data()\n",
    "\n",
    "\n",
    "usecols = obs.loc[obs.oname=='hds'].usecol.unique()\n",
    "cells = np.array([[int(i.split(\"-\")[-2]),int(i.split(\"-\")[-1])] for i in usecols])\n",
    "drop_rows=[]\n",
    "for usecol in usecols:\n",
    "    i,j = int(usecol.split(\"-\")[-2]),int(usecol.split(\"-\")[-1])\n",
    "    elev = top.max()#top[i,j]\n",
    "    obsnmes = obs.loc[(obs.usecol==usecol) & (obs.weight>0),\"obsnme\"].values\n",
    "    if len(obsnmes) == 0:\n",
    "        continue\n",
    "     # drop rows in data where value > elev\n",
    "    drop_rows.extend(data.loc[data.loc[:,obsnmes].gt(elev).any(axis=1)].index.tolist())\n",
    "\n",
    "len(list(set(drop_rows)))\n",
    "len(drop_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_redux = data.loc[keep_reals,:].copy()\n",
    "data_redux.drop(index=[i for i in drop_rows if i in data_redux.index], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst_freyberg.observation_data\n",
    "logcols =  obs.loc[(obs.usecol==\"gage-1\") & (obs.weight>0)].obsnme.tolist()\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "transforms = [\n",
    "    {\n",
    "        \"type\": \"log10\",\n",
    "        \"columns\": [\"part_time\"],# + logcols,\n",
    "    },\n",
    "    #{\n",
    "    #    \"type\": \"sklearn\",\n",
    "    #    \"columns\": [\"part_time\"],\n",
    "    #    \"estimator\": PowerTransformer(),\n",
    "    #    \"init_kwargs\": {\"method\": \"yeo-johnson\",\"standardize\": True}\n",
    "    #},\n",
    "    #{\n",
    "    #    \"type\": \"normal_score\",\n",
    "    #    \"columns\": obs.loc[obs.oname==\"hds\",\"obsnme\"].tolist(),\n",
    "    #    \n",
    "    #},\n",
    "    #{\n",
    "    #    \"type\": \"normal_score\",\n",
    "    #    \"columns\": obs.loc[obs.usecol==\"tailwater\"].obsnme.tolist(),#obs.loc[obs.oname==\"sfr\",\"obsnme\"].tolist(),\n",
    "    #    \n",
    "    #},\n",
    "    #{\n",
    "    #    \"type\": \"normal_score\",\n",
    "    #    \"columns\": obs.loc[obs.oname==\"sfr\",\"obsnme\"].tolist(),\n",
    "    #    \n",
    "    #},\n",
    "    {\n",
    "        \"type\": \"normal_score\",\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "dsi = DSI(pst=pst_freyberg,\n",
    "          data=data.copy(), #if you want to see the impact of not removing outliers, replace with: data.copy()\n",
    "          transforms=transforms,\n",
    "          energy_threshold=1.,\n",
    "          verbose=True)\n",
    "dsi.fit();\n",
    "\n",
    "\n",
    "t_d = \"dsi_template\"\n",
    "dsi.prepare_pestpp(t_d=t_d)\n",
    "\n",
    "\n",
    "# from pst_template copy exe files\n",
    "found = False\n",
    "for f in os.listdir(os.path.join(ies_d)):\n",
    "    if f.startswith(\"pestpp-ies\"):\n",
    "        shutil.copy2(os.path.join(ies_d,f),os.path.join(t_d,f))\n",
    "        found = True\n",
    "if not found:\n",
    "    raise Exception(\"couldn't find pestpp-ies binary in {0}\".format(ies_d))\n",
    "\n",
    "# load the control file\n",
    "pst = pyemu.Pst(os.path.join(t_d,\"dsi.pst\"))\n",
    "\n",
    "# lets specify the number of realizations to use; as usual this should be as many as you can afford\n",
    "pst.pestpp_options[\"ies_num_reals\"] = num_reals\n",
    "pst.pestpp_options[\"ies_drop_conflicts\"] = True\n",
    "\n",
    "#noise.to_csv(os.path.join(t_d,\"noise.csv\"))\n",
    "#pst.pestpp_options[\"ies_obs_en\"] = \"noise.csv\"\n",
    "newnoise.to_csv(os.path.join(t_d,\"extreme_noise.csv\"))\n",
    "pst.pestpp_options[\"ies_observation_ensemble\"] = \"extreme_noise.csv\"\n",
    "\n",
    "# set noptmax \n",
    "pst.control_data.noptmax = 5\n",
    "\n",
    "# and re-write\n",
    "pst.write(os.path.join(t_d,\"dsi.pst\"),version=2)\n",
    "\n",
    "# the master dir\n",
    "m_d = \"master_dsi_nst\"\n",
    "\n",
    "pvals = pd.read_csv(os.path.join(t_d, \"dsi_pars.csv\"), index_col=0)\n",
    "\n",
    "\n",
    "pyemu.os_utils.start_workers(\n",
    "    t_d,\"pestpp-ies\",\"dsi.pst\", num_workers=num_workers,\n",
    "    worker_root=\".\", master_dir=m_d, #port=_get_port(),\n",
    "    ppw_function=pyemu.helpers.dsi_pyworker,\n",
    "    ppw_kwargs={\n",
    "        \"dsi\": dsi, \"pvals\": pvals,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(m_d,\"dsi.pst\"))\n",
    "phidf = pst.ies.phiactual\n",
    "\n",
    "fig,ax=plt.subplots(1,1,figsize=(4,4))\n",
    "ax.plot(phidf.index,phidf['mean'],\"bo-\", label='dsi')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('phi')\n",
    "ax.set_xlabel('iteration')\n",
    "\n",
    "ax.text(0.7,0.9,f\"nnz_obs: {pst.nnz_obs}\\nphi_dsi: {phidf['mean'].iloc[-1]:.2f}\",\n",
    "        transform=ax.transAxes,ha=\"right\",va=\"top\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we achieved near noise levels of fit without too much effort. We would not want to fit any further...and we might even wish to not fit this well...But lets take the results of the last iteration and see how our predictions did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dsi_hist(m_d,pst,iteration=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so what has happened? Starting with the good news: `part_time` is behaving nicely now. That log-transform did the job. The same for the `hds` perdiction, DSi is slaying it! \n",
    "\n",
    "Bit off on the head/tailwater predicitons tho. We see a left leaning tail for DSI that was not ocurring with the full-order model...and was better represented without the transforms. This is an important detail: normal-score transformation introduces error! Specialy if the training data distribution is not a nice smooth statisticaly-relevant distribution. Beware of observations that have bounded distributions, and cases where the sample density does not provide a nice even distirbution acrsoss the enitre range or cases with long sparese tails. These will introduce \"noise\" into the mapping back and forth the data's distirbution and the normal-distribution. As we have seen, in some cases it can be more robust to not transform. Again, using more realizations in the training data should aid in obtaining a smoother normal-score transformer. Same as it ever was: use as many reals as you can...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a quick look at the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_oe_dsi = pst.ies.get(\"obsen\",5)\n",
    "noise_oe = pst.ies.noise.copy()\n",
    "fig = plot_tseries_ensembles(pt_oe_dsi, onames=[\"hds\",\"sfr\"], noise_oe=noise_oe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behaving a bit more \"realistic\". Still some weird responses in the head time series. Try running the last DSI history matching with the `data_redux` as training data. You should see a much more \"realistic\" time series behaviour. This suggest that, statisticaly, the measured data is insuficient to condition those forecasts. However, in the full-order model, physics does the job of translating that information to the forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final remarks\n",
    "\n",
    "Data space inversion provides a powerful tool for data assimilation and uncertainty quantification for cases in which model complexity and/or computational cost preclude the use of traditional methods. \n",
    "\n",
    " - use as many reals as possible when generating training data. The more the better.\n",
    " - the same applies for DSI conditioning with IES. The more reals the better.\n",
    "  - make sure the DSI generated prior reflects the physical model based prior. This may require a number of DSI realizations equal to or greater than the training data. Specially if using normal-score transformation.\n",
    " - if forecasts are leaning towards the ends (or beyond!) of the prior distribution, this is a red flag. It is probably worth revising the physics-based model prior parameter distributions and re-training the emulator.\n",
    " - don't overfit...#duh\n",
    "\n",
    " ### Benefits\n",
    " - extreme numerical efficiency. We only need a few hundred runs of the physics-based model as we are not using it for parameter adjustment. It is only used to construct/train the statistical model.\n",
    " - can use DSI as a verification of IES forecasts: are these too wide and/or not wide enough?\n",
    " - allows for parameter distributions of arbitrary complexity in the prior.\n",
    " - can be used for analyzing the worth of existing and as-of-yet uncollected data to reduce predictive uncertainty. With no assumption of linearity! (see ensemble dataworth notebook)\n",
    "\n",
    "### Drawbacks\n",
    " - if the relationship between the past and future are highly non-linear, it will struggle but then, so will any other method.\n",
    " - it is not possible to view the process-based model (eg MODFLOW) inputs that produce the DSI forecast posterior distribution, so can't \"see\" what model inputs might be causing extreme results. Howver, you can incluce model parameters as \"observations\" Although the direct link between parameter values and model ouputs may not be quite right, the statistical distribution will be informative.\n",
    " - when simulating future scenarios, you have to rerun the training ensemble through the process-based model and also rerun the DSI training process for each scenario. But you would also have to run the ensemble for each scenario with the physics-based model..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmdsitut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
