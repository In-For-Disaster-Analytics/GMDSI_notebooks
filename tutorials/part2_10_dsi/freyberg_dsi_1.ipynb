{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt;\n",
    "import psutil\n",
    "\n",
    "import sys\n",
    "import pyemu\n",
    "import flopy\n",
    "assert \"dependencies\" in flopy.__file__\n",
    "assert \"dependencies\" in pyemu.__file__\n",
    "sys.path.insert(0,\"..\")\n",
    "import herebedragons as hbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-space Inversion\n",
    "\n",
    "Data space inversion (DSI) enables the exploration of a model prediction's posterior distribution without requiring the exploration of the posterior distribution of model parameters. This is achieved by constructing a surrogate model using principal component analysis (PCA) of a covariance matrix of model outputs. This matrix links model outputs corresponding to field measurements with predictions of interest. The resulting predictions are then conditioned on real-world measurements of system behavior.\n",
    "\n",
    "DSI is a powerful tool for predictive uncertainty quantification, as it allows for the exploration of the uncertainty in model predictions without the need to explore the uncertainty in model parameters. This is particularly useful when the model is complex and the parameter space is high-dimensional. DSI can also be used to explore the sensitivity of model predictions to different types of data, and to identify the most informative data types for reducing predictive uncertainty.\n",
    "\n",
    "\n",
    "\n",
    "See the \"intro to DSI\" notebook for a more detailed explanation of the method, as well as the original papers [Sun and Durlofsky (2017)](https://doi.org/10.1007/s11004-016-9672-8), and subsequent variations (e.g., [Lima et al 2020](https://doi.org/10.1007/s10596-020-09933-w)). There are also [GMDSI webinars and lectures available on YouTube](https://www.youtube.com/watch?v=s2g3HaJa1Wk&t=1564s).\n",
    "\n",
    "In these notebooks we will focus on how to implement DSI for predictive uncertainty quantification, using pyEMU and PEST++. We will not discuss the maths behind the method in detail, but rather focus on how to use the tools to implement the method. For a review of the maths see the [\"intro to dsi\"](../part0_intro_to_dsi/intro_to_dsi.ipynb) notebook.\n",
    "\n",
    "## Getting ready\n",
    "\n",
    "Undertaking DSI relies on the existence of an ensemble of model-generated outputs (i.e., observations). These are generated by running the model with a range of parameter values, which are sampled from a prior distribution. Note that this prior does not need to be Gaussian, and each model \"parameterization\" can be as complex as the user desires. Generating the prior is the only time that the numerical model needs to be run. Ideally, the ensemble size should be as large as you can afford. However, once generated, the DSI model runs very quickly.\n",
    "\n",
    "Let us start by generating the ensemble of model outputs. We will make use of the prior ensemble generated in a previous tutorial. The next couple of cells load necessary dependencies and call a convenience function to prepare the PEST dataset folder for you. This is the same dataset that was constructed during the [\"freyberg ies\"](../part2_06_ies/freyberg_ies_1_basics.ipynb) tutorial. Simply press `shift+enter` to run the cells.\n",
    "\n",
    "Note: in our tutorial case, there is not actually much computational advantage in using the emulator (i.e., DSI) versus using the numerical model (i.e. Freyberg model). This is because the Freyberg model is very fast already. However, the DSI method is very useful for more computationally expensive models.\n",
    "\n",
    "..anyway...lez'go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the template directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the temporary working folder\n",
    "t_d = os.path.join('master_ies_1a')\n",
    "if os.path.exists(t_d):\n",
    "    shutil.rmtree(t_d)\n",
    "\n",
    "org_t_d = os.path.join(\"..\",\"part2_06_ies\",\"master_ies_1a\")\n",
    "if not os.path.exists(org_t_d):\n",
    "    raise Exception(f\"you need to run the {org_t_d} notebook\")\n",
    "shutil.copytree(org_t_d,t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us quickly load the Pst control file and remind ourselves of observations and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_name = os.path.join(t_d, \"freyberg_mf6.pst\")\n",
    "pst = pyemu.Pst(pst_name)\n",
    "pst.pestpp_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pst.pestpp_options['forecasts'].split(',')\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading the prior observation ensemble. Check how many realisations it has. These are our \"training data\". Let's use all of them. (if you like, you can experiment with what happens by using less realizations in the training dataset...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_name = pst_name.replace(\".pst\", \".0.obs.csv\")\n",
    "oe_pr = pyemu.ObservationEnsemble.from_csv(pst=pst, filename=pst_name.replace(\".pst\", \".0.obs.csv\"))\n",
    "oe_pt = pyemu.ObservationEnsemble.from_csv(pst=pst, filename=pst_name.replace(\".pst\", \".3.obs.csv\"))\n",
    "\n",
    "oe_pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reals = oe_pr.shape[0]\n",
    "num_reals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous notebooks, we set the observation weights to be \"balanced for visibility\". Thus they do not reflect a measure of measurement noise. However, we included the `standard_deviation` column in the PEST observation data section. Ensembles of observation noise are generated using the latter.\n",
    "\n",
    "Experience suggests that, when conditioning a Gaussian process model, there is not much advantage in weighting for visibility. Achieving model to measurement fits commensurate with noise is rarely an issue with these methods. In practice, it may be more convenient to assign weights explicitly as the inverse of the standard deviation of noise. This provides an easy way to verify if over-fitting is occurring (i.e Phi should be >= than the number of non-zero obs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs.loc[obs.weight>0, ['obsval','weight','standard_deviation']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change our observation weights to be the inverse of the standard deviation of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.loc[obs.weight>0,\"weight\"] = 1.0 / obs.loc[obs.weight>0,\"standard_deviation\"]\n",
    "assert obs.weight.sum()>0, \"no non-zero obs weights found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pyEMU, the `EnDS` class is the entry point for all things DSI. It is initialized with the PEST control file and the ensemble of model outputs. When initialized it prepares in memory the various components required for DSI and associated analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ends = pyemu.EnDS(pst=pst,\n",
    "                  sim_ensemble=oe_pr,\n",
    "                  predictions=predictions,\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is where things get fancy. We need to construct a surrogate model. We also need to construct a new PEST setup to condition the surrogate. Luckily, pyEMU has a function that does all the hard work for us!\n",
    "\n",
    "When calling `.prep_for_dsi()` on the `EnDS` object, pyemu will prepare a Pst object and folder with all the files required to run and condition the surrogate model using pestpp-ies. The new PEST control file is named `dsi.pst`. It contains all the observations that were included in the Pst object passed to `EnDS`. Parameters in the new control file are the vector of $\\mathbf{x}$, i.e. the surrogate model parameters.\n",
    "\n",
    "The `.prep_for_dsi()` method provides some optional arguments that enable the user to specify that observations should be subject to normal-score transformation, and to specify the use of truncated-SVD. The latter can be useful to reduce the dimensionality of the problem and for numerical stability. The former is often useful to improve the Gaussianity of the data - a condition on which these method relies. \n",
    "\n",
    "For now let us just use the defaults (i.e. no normal-score transformation and no truncated-SVD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = \"dsi_template\"\n",
    "pst_dsi = ends.prep_for_dsi(t_d=t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One extra step for our case, because `.prep_for_dsi` does not copy over executables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pst_template copy exe files\n",
    "for f in os.listdir(os.path.join(\"pst_template\")):\n",
    "    if f.startswith(\"pestpp-ies.\"):\n",
    "        shutil.copy2(os.path.join(\"pst_template\",f),os.path.join(t_d,f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at whats in this new folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the various components of the `dsi.pst` version 2 control file. Files that start with `dsi_` are the model emulator input and output files. \n",
    "\n",
    "The `dsi_pr_mean.csv` and `dsi_proj_mat.jcb` are inputs required by the emulator. They are the prior mean observation values $\\bar{\\mathbf{d}}$ and the matrix of the square root of observation covariance matrix $\\mathbf{C}_d^{1/2}$. (see the \"intro to dsi\" notebook for more details)\n",
    "The `dsi_sim_vals.csv` file contains the emulator generated observation values using the input parameters, $\\mathbf{x}$, contained in `dsi_pars.csv`. \n",
    "The `forward_run.py` script contains code to read input files, run the emulator and record outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at how npar has changed. The number of parameters in the original Pst object is the number of parameters in the model. The number of parameters in the new Pst object is the number of principal components used in the emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_dsi.npar_adj, pst.npar_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify we have the same number of observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_dsi.nobs, pst.nobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_dsi.nnz_obs, pst.nnz_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right on! We are ready to get cracking. Let's run pestpp-ies and see what we get.\n",
    "\n",
    "#ATTENTION!\n",
    "\n",
    "As always, set the number of workers according to your resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the number of realizations and how many workers you have at your disposal, the next cell may take a while to run. Although the emulator model is super fast, we are running it many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the control file\n",
    "pst = pyemu.Pst(os.path.join(t_d,\"dsi.pst\"))\n",
    "\n",
    "# lets specify the number of realizations to use; as usual this should be as many as you can afford\n",
    "pst.pestpp_options[\"ies_num_reals\"] = num_reals\n",
    "\n",
    "# and a options to reduce the number of lost reals\n",
    "pst.pestpp_options[\"overdue_giveup_fac\"] = 1e30\n",
    "pst.pestpp_options[\"overdue_giveup_minutes\"] = 1e30\n",
    "\n",
    "# set noptmax \n",
    "pst.control_data.noptmax = 5\n",
    "\n",
    "# and re-write\n",
    "pst.write(os.path.join(t_d,\"dsi.pst\"),version=2)\n",
    "\n",
    "# the master dir\n",
    "m_d = \"master_dsi\"\n",
    "pyemu.os_utils.start_workers(t_d,\"pestpp-ies\",\"dsi.pst\",num_workers=num_workers,worker_root='.',\n",
    "                                master_dir=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right on. Let's check the evolution of the objective function. We should see it decreasing as the emulator is conditioned on the observations, but we don't want it to become less than nnzobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_d = os.path.join(\".\", \"master_dsi\")\n",
    "pst = pyemu.Pst(os.path.join(m_d,\"dsi.pst\"))\n",
    "phidf = pd.read_csv(os.path.join(m_d,\"dsi.phi.actual.csv\"),index_col=0)\n",
    "\n",
    "fig,ax=plt.subplots(1,1,figsize=(4,4))\n",
    "ax.plot(phidf.index,phidf['mean'],\"bo-\", label='dsi')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('phi')\n",
    "ax.set_xlabel('iteration')\n",
    "\n",
    "ax.text(0.7,0.9,f\"nnz_obs: {pst.nnz_obs}\\nphi_dsi: {phidf['mean'].iloc[-1]:.2f}\",\n",
    "        transform=ax.transAxes,ha=\"right\",va=\"top\")\n",
    "\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as usual, we bring this back to the predictions. How has DSI performed? While we are at it, let's compare to the numerical model derived predictions we obtained in the \"freyberg ies\" notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dsi_hist(m_d,pst,iteration=3):\n",
    "    pr_oe_dsi = pd.read_csv(os.path.join(m_d,\"dsi.0.obs.csv\"),index_col=0)\n",
    "    pt_oe_dsi = pd.read_csv(os.path.join(m_d, f\"dsi.{iteration}.obs.csv\"), index_col=0)\n",
    "\n",
    "    #pv = pyemu.ObservationEnsemble(pst=pst,df=oe_pt).phi_vector\n",
    "    #pv_dsi = pyemu.ObservationEnsemble(pst=pst, df=pt_oe_dsi).phi_vector\n",
    "\n",
    "    fig,axes = plt.subplots(len(predictions),1,figsize=(7,7))\n",
    "    for p,ax in zip(predictions,axes):\n",
    "            #calculate consistent bin edges\n",
    "            bins = np.linspace(\n",
    "                    min(oe_pr.loc[:,p].values.min(),\n",
    "                        oe_pt.loc[:,p].values.min(),\n",
    "                        pr_oe_dsi.loc[:,p].values.min(),\n",
    "                        pt_oe_dsi.loc[:,p].values.min()),\n",
    "                    max(oe_pr.loc[:,p].values.max(),\n",
    "                        oe_pt.loc[:,p].values.max(),\n",
    "                        pr_oe_dsi.loc[:,p].values.max(),\n",
    "                        pt_oe_dsi.loc[:,p].values.max()),\n",
    "                    50)\n",
    "\n",
    "            ax.hist(oe_pr.loc[:,p].values,alpha=0.5,facecolor=\"0.5\",density=True,label=\"prior\",bins=bins)\n",
    "            ax.hist(oe_pt.loc[:, p].values,  alpha=0.5, facecolor=\"b\",density=True,label=\"posterior\",bins=bins)\n",
    "            ax.hist(pr_oe_dsi.loc[:, p].values,  facecolor=\"none\",hatch=\"/\",edgecolor=\"0.5\",lw=0.5,density=True,label=\"dsi prior\",bins=bins)\n",
    "            ax.hist(pt_oe_dsi.loc[:, p].values,  facecolor=\"none\",density=True,hatch=\"/\",edgecolor=\"b\",lw=.5,label=\"dsi posterior\",bins=bins)\n",
    "            \n",
    "            fval = pst.observation_data.loc[p,\"obsval\"]\n",
    "            ax.plot([fval,fval],ax.get_ylim(),\"r-\",label='truth')\n",
    "            \n",
    "            ax.set_title(p,loc=\"left\")\n",
    "            ax.legend(loc=\"upper right\")\n",
    "            ax.set_yticks([])\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_d = os.path.join(\".\", \"master_dsi\")\n",
    "pst = pyemu.Pst(os.path.join(m_d,\"dsi.pst\"))\n",
    "\n",
    "iteration=phidf.index.values[-1]\n",
    "plot_dsi_hist(m_d,pst,iteration=iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby. Prediction variance has decreased for most forecasts. And the posterior distributions all capture the truth. \n",
    "\n",
    "There is a noticeable issue for the particle travel time prediction: the model emulator is predicting physically impossible values (i.e. negative times). On top of that, the physics-based model prior and posterior show a distinctly skewed distribution, whilst the emulator derived output are symmetric. This is due to the Gaussian assumption of the emulator. To avoid it, we can (and should!) apply transformation of the data. We shall do that in a minute. \n",
    "\n",
    "First, lets look at another common issue: saw-tooth patterns in time-series outputs. Make a quick plot of the future time series outputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tseries_ensembles(pt_oe, onames=[\"hds\",\"sfr\"]):\n",
    "    pst.try_parse_name_metadata()\n",
    "    # get the observation data from the control file and select \n",
    "    obs = pst.observation_data.copy()\n",
    "    # onames provided in oname argument\n",
    "    obs = obs.loc[obs.oname.apply(lambda x: x in onames)]\n",
    "    # only non-zero observations\n",
    "    obs = obs.loc[obs.obgnme.apply(lambda x: x in pst.nnz_obs_groups),:]\n",
    "    # make a plot\n",
    "    ogs = obs.obgnme.unique()\n",
    "    fig,axes = plt.subplots(len(ogs),1,figsize=(7,2*len(ogs)))\n",
    "    ogs.sort()\n",
    "    # for each observation group (i.e. timeseries)\n",
    "    for ax,og in zip(axes,ogs):\n",
    "        # get values for x axis\n",
    "        oobs = obs.loc[obs.obgnme==og,:].copy()\n",
    "        oobs.loc[:,\"time\"] = oobs.time.astype(float)\n",
    "        oobs.sort_values(by=\"time\",inplace=True)\n",
    "        tvals = oobs.time.values\n",
    "        onames = oobs.obsnme.values\n",
    "        ax.plot(oobs.time,oobs.obsval,color=\"fuchsia\",lw=2,zorder=100)\n",
    "        # plot prior\n",
    "        #[ax.plot(tvals,pr_oe.loc[i,onames].values,\"0.5\",lw=0.5,alpha=0.5) for i in pr_oe.index]\n",
    "        # plot posterior\n",
    "        [ax.plot(tvals,pt_oe.loc[i,onames].values,\"b\",lw=0.5,alpha=0.5) for i in pt_oe.index]\n",
    "        # plot measured+noise \n",
    "        oobs = oobs.loc[oobs.weight>0,:]\n",
    "        tvals = oobs.time.values\n",
    "        onames = oobs.obsnme.values\n",
    "        #[ax.plot(tvals,noise.loc[i,onames].values,\"r\",lw=0.5,alpha=0.5,zorder=0) for i in noise.index]\n",
    "        ax.plot(oobs.time,oobs.obsval,\"r-o\",lw=2,zorder=100)\n",
    "        ax.set_title(og,loc=\"left\")\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell plots the posterior of three forecast time-series. As you can see, DSI generated outputs are nice and smooth in the historical period. However, the same cannot be said for the forecast period, where a \"saw-toothed\" behaviour can be seen, with the time-series jumping up and down. It is specially evident in the early-future for site `trgw-0-3-8`. This too will be mitigated by using normal-score transformation of model derived outputs prior to undertaking DSI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration=phidf.index.values[-1]\n",
    "pt_oe_dsi = pd.read_csv(os.path.join(m_d, f\"dsi.{iteration}.obs.csv\"), index_col=0)\n",
    "fig = plot_tseries_ensembles(pt_oe_dsi, onames=[\"hds\",\"sfr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DSI with normal-score transformation\n",
    "\n",
    "We will now repeat all of the above, but using the normal-score transformation (NST) option when calling `.prep_for_dsi()`. This option will first apply a normal-score transform to all model derived outputs in the training dataset. Transformed observation values will have a distribution which is more similar to Gaussian, thus better respecting the assumptions on which DSI relies. Then, during the DSI forward run, emulator-derived outputs are back-transformed into the original data space, so as to allow for direct comparison to measured values.\n",
    "\n",
    "In the `pyemu` python implementation this comes at a slight cost of increased run time, both in setting up the emulator, as well as in the forward run. The latter may cost 10 seconds instead of 5. Sorry about that. \n",
    "\n",
    "Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = \"dsi_template\"\n",
    "pst_dsi = ends.prep_for_dsi(t_d=t_d,apply_normal_score_transform=True)\n",
    "\n",
    "# from pst_template copy exe files\n",
    "for f in os.listdir(os.path.join(\"pst_template\")):\n",
    "    if f.startswith(\"pestpp-ies.\"):\n",
    "        shutil.copy2(os.path.join(\"pst_template\",f),os.path.join(t_d,f))\n",
    "\n",
    "# load the control file\n",
    "pst = pyemu.Pst(os.path.join(t_d,\"dsi.pst\"))\n",
    "\n",
    "# lets specify the number of realizations to use; as usual this should be as many as you can afford\n",
    "pst.pestpp_options[\"ies_num_reals\"] = num_reals\n",
    "\n",
    "# and a options to reduce the number of lost reals\n",
    "pst.pestpp_options[\"overdue_giveup_fac\"] = 1e30\n",
    "pst.pestpp_options[\"overdue_giveup_minutes\"] = 1e30\n",
    "\n",
    "# set noptmax \n",
    "pst.control_data.noptmax = 5\n",
    "\n",
    "# and re-write\n",
    "pst.write(os.path.join(t_d,\"dsi.pst\"),version=2)\n",
    "\n",
    "# the master dir\n",
    "m_d = \"master_dsi_nst\"\n",
    "pyemu.os_utils.start_workers(t_d,\"pestpp-ies\",\"dsi.pst\",num_workers=num_workers,worker_root='.',\n",
    "                                master_dir=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the Phi progressed during that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_d = os.path.join(\".\", \"master_dsi_nst\")\n",
    "pst = pyemu.Pst(os.path.join(m_d,\"dsi.pst\"))\n",
    "phidf = pd.read_csv(os.path.join(m_d,\"dsi.phi.actual.csv\"),index_col=0)\n",
    "fig,ax=plt.subplots(1,1,figsize=(4,4))\n",
    "ax.plot(phidf.index,phidf['mean'],\"bo-\", label='dsi')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('phi')\n",
    "ax.set_xlabel('iteration')\n",
    "\n",
    "ax.text(0.7,0.9,f\"nnz_obs: {pst.nnz_obs}\\nphi_dsi: {phidf['mean'].iloc[-1]:.2f}\",\n",
    "        transform=ax.transAxes,ha=\"right\",va=\"top\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we achieved near noise levels of fit without too much effort. We would not want to fit any further...and we might even wish to not fit this well...But lets take the results of the last iteration and see how our predictions did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration=phidf.index.values[-1]\n",
    "pst = pyemu.Pst(os.path.join(m_d,\"dsi.pst\"))\n",
    "plot_dsi_hist(m_d,pst,iteration=iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! #winning. This time around, both the DSI prior and posterior look a lot more similar to those we obtained with the Freyberg model. On top of that, we now see the particle time distribution no longer has physically infeasible values, and it follows the same log distribution with a long tail.\n",
    "\n",
    "Finally, lets take a look at the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_oe_dsi = pd.read_csv(os.path.join(m_d, f\"dsi.{iteration}.obs.csv\"), index_col=0)\n",
    "fig = plot_tseries_ensembles(pt_oe_dsi, onames=[\"hds\",\"sfr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go. A lot less \"saw toothing\", with somewhat smoother time-series in the forecast period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final remarks\n",
    "\n",
    " - use as many reals as possible when generating training data. The more the better.\n",
    " - the same applies for DSI conditioning with IES. The more reals the better.\n",
    "  - make sure the DSI generated prior reflects the physical model based prior. This may require a number of DSI realizations equal to or greater than the training data. Specially if using normal-score transformation.\n",
    " - if forecasts are leaning towards the ends (or beyond!) of the prior distribution, this is a red flag. It is probably worth revising the physics-based model prior parameter distributions and re-training the emulator.\n",
    " - don't overfit...#duh\n",
    "\n",
    " ### Benefits:\n",
    " - extreme numerical efficiency. We only need a few hundred runs of the physics-based model as we are not using it for parameter adjustment. It is only used to construct/train the statistical model.\n",
    " - can use DSI as a verification of IES forecasts: are these too wide and/or not wide enough?\n",
    " - allows for parameter distributions of arbitrary complexity. \n",
    " - can be used for analyzing the worth of existing and as-of-yet uncollected data to reduce predictive uncertainty. With no assumption of linearity! (see next notebook)\n",
    "\n",
    "### Drawbacks:\n",
    " - if the relationship between the past and future are highly non-linear, it will not work. However, this will be an issue if using IES with a physics-based model anyway...\n",
    " - it is not possible to query the parameters that result in extreme forecasts\n",
    " - it is important that the physics-based model be complex enough to ensure that the connection between the past and the forecast have integrity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmdsitut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
